---
title: Applied Machine Learning (AML)
icon: material/slot-machine
hide:
  - toc
---

<div class="hero-banner" style="background-image: url('images/background_aml.png')"></div>

<h1>
  <span style="color:#87ceeb;">üêé A</span>pplied 
  <span style="color:#87ceeb;">M</span>achine 
  <span style="color:#87ceeb;">L</span>earning 
  (<span style="color:#87ceeb;">AML</span>)
</h1>
<br>
>üåü **"Success is not an accident. It‚Äôs hard work, learning, and persistence."**
>
>üöÄ **"Start where you are. Use what you have. Do what you can."**

![Static Badge](https://img.shields.io/badge/lecture_notes-blue?style=for-the-badge&logo=notepad%2B%2B&logoColor=white)
![Static Badge](https://img.shields.io/badge/Code-red?style=for-the-badge&logo=lintcode&logoColor=white)
![Static Badge](https://img.shields.io/badge/Materials-yellow?style=for-the-badge&logo=bookstack)

## I. Course Introduction
---

**M√¥ t·∫£.** Applied Machine Learning (AML) ƒë∆∞·ª£c ghi ch√∫ v√† t·ªïng h·ª£p theo ƒë·ªãnh d·∫°ng m·ªôt kh√≥a h·ªçc, nh·∫±m cung c·∫•p cho ng∆∞·ªùi h·ªçc c√°i nh√¨n t·ªïng quan v√† to√†n di·ªán v·ªÅ lƒ©nh v·ª±c Machine Learning (ML) - t·ª´ l√Ω thuy·∫øt c·ªët l√µi ƒë·∫øn th·ª±c h√†nh tri·ªÉn khai. To√†n b·ªô n·ªôi dung t·∫≠p trung v√†o c√°c k·ªπ thu·∫≠t v√† thu·∫≠t to√°n n·ªÅn t·∫£ng, gi√∫p b·∫°n n·∫Øm v·ªØng c√°ch th·ª©c thu th·∫≠p d·ªØ li·ªáu, ti·ªÅn x·ª≠ l√Ω, v√† x√¢y d·ª±ng c√°c m√¥ h√¨nh h·ªçc m√°y m·ªôt c√°ch b√†i b·∫£n v√† c√≥ h·ªá th·ªëng. Kh√≥a h·ªçc kh√¥ng ch·ªâ ch√∫ tr·ªçng v√†o ki·∫øn th·ª©c l√Ω thuy·∫øt m√† c√≤n h∆∞·ªõng d·∫´n ng∆∞·ªùi h·ªçc t·ª´ng b∆∞·ªõc tri·ªÉn khai, hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh ML t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi, qua c√°c b√†i t·∫≠p th·ª±c h√†nh v√† d·ª± √°n th·ª±c t·∫ø.

**ƒê·ªëi t∆∞·ª£ng.** Kh√≥a h·ªçc ph√π h·ª£p v·ªõi ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu l√†m quen v·ªõi Tr√≠ tu·ªá Nh√¢n t·∫°o v√† H·ªçc m√°y. Ngo√†i ra, nh·ªØng ng∆∞·ªùi ƒë√£ c√≥ ki·∫øn th·ª©c n·ªÅn t·∫£ng c≈©ng c√≥ th·ªÉ s·ª≠ d·ª•ng t√†i li·ªáu n√†y nh∆∞ m·ªôt ngu·ªìn tham kh·∫£o h·ªá th·ªëng v√† th·ª±c ti·ªÖn.

**C·∫•u tr√∫c kh√≥a h·ªçc.** Kh√≥a h·ªçc ƒë∆∞·ª£c t·ªïng h·ª£p v√† chia nh·ªè th√†nh 5 Module ch√≠nh, x√¢y d·ª±ng t·ª´ n·ªÅn t·∫£ng l√Ω thuy·∫øt cho ƒë·∫øn th·ª±c h√†nh tri·ªÉn khai to√†n b·ªô h·ªá th·ªëng ML trong th·ª±c t·∫ø. C·ª• th·ªÉ:

- Module 01. Introduction to ML & Development Environment (T·ªïng quan v·ªÅ H·ªçc m√°y v√† M√¥i tr∆∞·ªùng ph√°t tri·ªÉn)
- Module 02. Data Preprocessing & Exploratory Data Analysis (Ti·ªÅn x·ª≠ l√Ω v√† Kh√°m ph√° d·ªØ li·ªáu)
- Module 03. Supervised Learning Algorithms (C√°c thu·∫≠t to√°n h·ªçc c√≥ gi√°m s√°t)
- Module 04. Unsupervised Learning Algorithms (C√°c thu·∫≠t to√°n h·ªçc kh√¥ng gi√°m s√°t)
- Module 05: ML Pipelines & Deployment (ƒê∆∞·ªùng ·ªëng ML v√† Tri·ªÉn khai h·ªá th·ªëng)

Chi ti·∫øt n·ªôi dung b√†i h·ªçc c·ªßa t·ª´ng Module ƒë∆∞·ª£c m√¥ t·∫£ chi ti·∫øt trong [[**syllabus**](#iv-syllabus)]

**Li√™n k·∫øt nhanh:** To√†n b·ªô n·ªôi dung kh√≥a h·ªçc bao g·ªìm ghi ch√∫ b√†i h·ªçc (notes), m√£ ch∆∞∆°ng tr√¨nh (code) v√† t√†i li·ªáu tham kh·∫£o (materials) ƒë∆∞·ª£c t·ªïng h·ª£p v√† c√≥ th·ªÉ truy c·∫≠p nhanh t·∫°i nh·ªØng ƒë·ªãa ch·ªâ sau:

- [**Lecture notes**](https://)
- [**Code for Course**](https://)
- [**Materials for Course**](https://)

## II. What will you learn?
---

Sau khi ƒë·ªçc xong to√†n b·ªô kh√≥a h·ªçc n√†y, b·∫°n c√≥ th·ªÉ thu th·∫≠p ƒë∆∞·ª£c nh·ªØng tri th·ª©c d∆∞·ªõi ƒë√¢y: 

- N·∫Øm v·ªØng c√°c thu·∫≠t to√°n h·ªçc m√°y c∆° b·∫£n nh∆∞ Linear Regression, Support Vector Machine, Decision Trees, K-mean Clustering, v.v.

- Th√†nh th·∫°o quy tr√¨nh x√¢y d·ª±ng m√¥ h√¨nh ML t·ª´ d·ªØ li·ªáu th√¥ ƒë·∫øn ƒë√°nh gi√° k·∫øt qu·∫£.

- Hi·ªÉu v√† √°p d·ª•ng k·ªπ thu·∫≠t thu th·∫≠p, ti·ªÅn x·ª≠ l√Ω v√† tr·ª±c quan h√≥a d·ªØ li·ªáu.

- Th·ª±c h√†nh tri·ªÉn khai m√¥ h√¨nh b·∫±ng Python v√† c√°c th∆∞ vi·ªán nh∆∞: Scikit-learn, Numpy, Pandas, Matplotlib, Seaborn, v.v.

- √Åp d·ª•ng m√¥ h√¨nh v√†o c√°c b√†i to√°n th·ª±c t·∫ø qua mini-projects v√† case studies.

## III. Requirements
---

ƒê·ªÉ c√≥ th·ªÉ ti·∫øp c·∫≠n n·ªôi dung c·ªßa to√†n b·ªô kh√≥a h·ªçc m·ªôt c√°ch d·ªÖ d√†ng, ng∆∞·ªùi ƒë·ªçc c·∫ßn c√≥ s·∫µn m·ªôt s·ªë ki·∫øn th·ª©c n·ªÅn t·∫£ng nh∆∞:

- L·∫≠p tr√¨nh Python c∆° b·∫£n: Th√†nh th·∫°o c√°c c·∫•u tr√∫c d·ªØ li·ªáu c∆° b·∫£n, v√≤ng l·∫∑p, h√†m v√† thao t√°c x·ª≠ l√Ω d·ªØ li·ªáu v·ªõi th∆∞ vi·ªán nh∆∞ numpy v√† pandas.

- To√°n h·ªçc n·ªÅn t·∫£ng: C√≥ ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ ƒë·∫°i s·ªë tuy·∫øn t√≠nh, gi·∫£i th√≠ch v√† x√°c su·∫•t th·ªëng k√™.

- K·ªπ nƒÉng t·ª± h·ªçc v√† gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ: S·∫µn s√†ng ti·∫øp c·∫≠n t√†i li·ªáu h·ªçc thu·∫≠t, nghi√™n c·ª©u thu·∫≠t to√°n, v√† th·ª≠ nghi·ªám m√¥ h√¨nh trong m√¥i tr∆∞·ªùng th·ª±c t·∫ø.


## IV. Syllabus
---

| Module | Topic | Content & Key Concepts | 
| :--- | :--- | :--- |
| *Module 01 <br> Overview & Environment* | **Lecture 01. Introduction to ML** <br> [[notes](https://)] [[code](https://)] | <ul><li>ƒê·ªãnh nghƒ©a & Ph√¢n lo·∫°i: Supervised, Unsupervised, RL</li><li>Th√°ch th·ª©c trong tri·ªÉn khai th·ª±c t·∫ø</li><li>Quy tr√¨nh ph√°t tri·ªÉn d·ª± √°n ML chu·∫©n</li></ul> |
| | **Lecture 02. ML Development Roadmap** | <ul><li>V√≤ng ƒë·ªùi ph√°t tri·ªÉn (ML Lifecycle)</li><li>C√°c y·∫øu t·ªë c·ªët l√µi: Data, Model, Compute</li><li>Gi·ªõi thi·ªáu v·ªÅ MLOps c∆° b·∫£n</li></ul> |
| | **Lecture 03. Python for Data Science** | <ul><li>Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng: Anaconda, Jupyter, Colab</li><li>H·ªá sinh th√°i: NumPy (ƒê·∫°i s·ªë), Pandas (D·ªØ li·ªáu), Matplotlib (V·∫Ω bi·ªÉu ƒë·ªì)</li></ul> |
| | | | |
| *Module 02 <br> Preprocessing & EDA* | **Lecture 04. Data Preprocessing Strategy** | <ul><li>T·∫ßm quan tr·ªçng c·ªßa ti·ªÅn x·ª≠ l√Ω</li><li>C√°c chi·∫øn l∆∞·ª£c l√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu</li><li>X·ª≠ l√Ω d·ªØ li·ªáu m·∫•t c√¢n b·∫±ng (Imbalanced Data)</li></ul> |
| | **Lecture 05. Cleaning & Missing Values** | <ul><li>Ph√°t hi·ªán v√† x·ª≠ l√Ω Outlier (Ngo·∫°i lai)</li><li>K·ªπ thu·∫≠t Imputation (ƒêi·ªÅn khuy·∫øt thi·∫øu)</li><li>L√†m s·∫°ch d·ªØ li·ªáu tr√πng l·∫∑p</li></ul> |
| | **Lecture 06. Feature Engineering** | <ul><li>Feature Selection (RFE, Feature Importance)</li><li>Feature Transformation (Scaling, Encoding)</li><li>Dimensionality Reduction c∆° b·∫£n</li></ul> |
| | | | |
| *Module 03 <br> Supervised Learning* | **Lecture 07. Linear Models** | <ul><li>Linear Regression & Logistic Regression</li><li>Gradient Descent & Normal Equation</li><li>Regularization: Ridge, Lasso, Elastic Net</li></ul> |
| | **Lecture 08. Support Vector Machines** | <ul><li>Nguy√™n l√Ω bi√™n c·ª©ng (Hard Margin) & bi√™n m·ªÅm (Soft Margin)</li><li>Kernel Trick & b√†i to√°n phi tuy·∫øn</li></ul> |
| | **Lecture 09. Tree-based Models** | <ul><li>C·∫•u tr√∫c Decision Tree (CART, ID3)</li><li>Ensemble Learning: Random Forest, Gradient Boosting</li><li>Overfitting & Pruning Strategies</li></ul> |
| | | | |
| *Module 04 <br> Unsupervised Learning* | **Lecture 10. Clustering Algorithms** | <ul><li>Ph√¢n c·ª•m K-means & K-medoids</li><li>Ph√¢n c·ª•m m·∫≠t ƒë·ªô (DBSCAN)</li><li>ƒê√°nh gi√° hi·ªáu qu·∫£ ph√¢n c·ª•m (Silhouette Score)</li></ul> |
| | **Lecture 11. Dimensionality Reduction** | <ul><li>Principal Component Analysis (PCA)</li><li>t-SNE & Manifold Learning</li></ul> |
| | | | |
| *Module 05 <br> Pipelines & Deployment* | **Lecture 12. Model Evaluation & Tuning** | <ul><li>Cross-Validation (K-Fold, Stratified)</li><li>Hyperparameter Tuning (GridSearch, RandomSearch)</li><li>Metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC</li></ul> |
| | **Lecture 13. ML Pipelines & Deployment** | <ul><li>X√¢y d·ª±ng Pipeline v·ªõi Scikit-learn</li><li>L∆∞u tr·ªØ m√¥ h√¨nh (Pickle, Joblib)</li><li>Chi·∫øn l∆∞·ª£c tri·ªÉn khai c∆° b·∫£n (API Serving)</li></ul> |

## V. Materials
---