---
title: Applied Machine Learning (AML)
hide:
    - navigation
---

<div style="
    background-image: url('images/background_aml.png');
    background-size: cover;
    background-position: center;
    background-repeat: no-repeat;
    min-height: 60vh;
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    color: white;
    text-align: center;
    padding: 40px 20px;
    margin-bottom: 30px;
">
</div>

<h1>
  <span style="color:#87ceeb;">üêé A</span>pplied 
  <span style="color:#87ceeb;">M</span>achine 
  <span style="color:#87ceeb;">L</span>earning 
  (<span style="color:#87ceeb;">AML</span>)
</h1>

---
>üåü **"Success is not an accident. It‚Äôs hard work, learning, and persistence."**
>
>üöÄ **"Start where you are. Use what you have. Do what you can."**

![Static Badge](https://img.shields.io/badge/lecture-notes-blue?style=for-the-badge&logo=notepad%2B%2B&logoColor=white)
![Static Badge](https://img.shields.io/badge/Code-for_course-red?style=for-the-badge&logo=lintcode&logoColor=brown)
![Static Badge](https://img.shields.io/badge/Materials-for_course-yellow?style=for-the-badge&logo=bookstack)

## I. Course Introduction

**M√¥ t·∫£.** Applied Machine Learning (AML) ƒë∆∞·ª£c ghi ch√∫ v√† t·ªïng h·ª£p theo ƒë·ªãnh d·∫°ng m·ªôt kh√≥a h·ªçc, nh·∫±m cung c·∫•p cho ng∆∞·ªùi h·ªçc c√°i nh√¨n t·ªïng quan v√† to√†n di·ªán v·ªÅ lƒ©nh v·ª±c Machine Learning (ML) - t·ª´ l√Ω thuy·∫øt c·ªët l√µi ƒë·∫øn th·ª±c h√†nh tri·ªÉn khai. To√†n b·ªô n·ªôi dung t·∫≠p trung v√†o c√°c k·ªπ thu·∫≠t v√† thu·∫≠t to√°n n·ªÅn t·∫£ng, gi√∫p b·∫°n n·∫Øm v·ªØng c√°ch th·ª©c thu th·∫≠p d·ªØ li·ªáu, ti·ªÅn x·ª≠ l√Ω, v√† x√¢y d·ª±ng c√°c m√¥ h√¨nh h·ªçc m√°y m·ªôt c√°ch b√†i b·∫£n v√† c√≥ h·ªá th·ªëng. Kh√≥a h·ªçc kh√¥ng ch·ªâ ch√∫ tr·ªçng v√†o ki·∫øn th·ª©c l√Ω thuy·∫øt m√† c√≤n h∆∞·ªõng d·∫´n ng∆∞·ªùi h·ªçc t·ª´ng b∆∞·ªõc tri·ªÉn khai, hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh ML t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi, qua c√°c b√†i t·∫≠p th·ª±c h√†nh v√† d·ª± √°n th·ª±c t·∫ø.

**ƒê·ªëi t∆∞·ª£ng.** Kh√≥a h·ªçc ph√π h·ª£p v·ªõi ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu l√†m quen v·ªõi Tr√≠ tu·ªá Nh√¢n t·∫°o v√† H·ªçc m√°y. Ngo√†i ra, nh·ªØng ng∆∞·ªùi ƒë√£ c√≥ ki·∫øn th·ª©c n·ªÅn t·∫£ng c≈©ng c√≥ th·ªÉ s·ª≠ d·ª•ng t√†i li·ªáu n√†y nh∆∞ m·ªôt ngu·ªìn tham kh·∫£o h·ªá th·ªëng v√† th·ª±c ti·ªÖn.

**C·∫•u tr√∫c kh√≥a h·ªçc.** Kh√≥a h·ªçc ƒë∆∞·ª£c t·ªïng h·ª£p v√† chia nh·ªè th√†nh 5 Module ch√≠nh - t·ª´ n·ªÅn t·∫£ng l√Ω thuy·∫øt cho ƒë·∫øn th·ª±c h√†nh tri·ªÉn khai to√†n b·ªô h·ªá th·ªëng ML trong th·ª±c t·∫ø. C·ª• th·ªÉ:

- Module 01. Introduction to ML & Development Environment (T·ªïng quan v·ªÅ H·ªçc m√°y v√† M√¥i tr∆∞·ªùng ph√°t tri·ªÉn)
- Module 02. Data Preprocessing & Exploratory Data Analysis (Ti·ªÅn x·ª≠ l√Ω v√† Kh√°m ph√° d·ªØ li·ªáu)
- Module 03. Supervised Learning Algorithms (C√°c thu·∫≠t to√°n h·ªçc c√≥ gi√°m s√°t)
- Module 04. Unsupervised Learning Algorithms (C√°c thu·∫≠t to√°n h·ªçc kh√¥ng gi√°m s√°t)
- Module 5: ML Pipelines & Deployment (ƒê∆∞·ªùng ·ªëng ML v√† Tri·ªÉn khai h·ªá th·ªëng)

Chi ti·∫øt n·ªôi dung b√†i h·ªçc c·ªßa t·ª´ng Module ƒë∆∞·ª£c m√¥ t·∫£ chi ti·∫øt trong [[**syllabus**](#iv-syllabus)]

**Li√™n k·∫øt nhanh:** To√†n b·ªô n·ªôi dung kh√≥a h·ªçc bao g·ªìm ghi ch√∫ b√†i h·ªçc (notes), m√£ ch∆∞∆°ng tr√¨nh (code) v√† t√†i li·ªáu tham kh·∫£o (materials) ƒë∆∞·ª£c t·ªïng h·ª£p v√† c√≥ th·ªÉ truy c·∫≠p nhanh t·∫°i nh·ªØng ƒë·ªãa ch·ªâ sau:

- [**Lecture notes**](https://)
- [**Code for Course**](https://)
- [**Materials for Course**](https://)

---
## II. What will you learn ?

Sau khi ƒë·ªçc xong to√†n b·ªô kh√≥a h·ªçc n√†y, b·∫°n c√≥ th·ªÉ thu th·∫≠p ƒë∆∞·ª£c nh·ªØng tri th·ª©c d∆∞·ªõi ƒë√¢y: 

- N·∫Øm v·ªØng c√°c thu·∫≠t to√°n h·ªçc m√°y c∆° b·∫£n nh∆∞ Linear Regression, Support Vector Machine, Decision Trees, K-mean Clustering, $\ldots$

- Th√†nh th·∫°o quy tr√¨nh x√¢y d·ª±ng m√¥ h√¨nh ML t·ª´ d·ªØ li·ªáu th√¥ ƒë·∫øn ƒë√°nh gi√° k·∫øt qu·∫£.

- Hi·ªÉu v√† √°p d·ª•ng k·ªπ thu·∫≠t thu th·∫≠p, ti·ªÅn x·ª≠ l√Ω v√† tr·ª±c quan h√≥a d·ªØ li·ªáu.

- Th·ª±c h√†nh tri·ªÉn khai m√¥ h√¨nh b·∫±ng Python v√† c√°c th∆∞ vi·ªán nh∆∞: Scikit-learn, Numpy, Pandas, Matplotlib, Seaborn, $\ldots$

- √Åp d·ª•ng m√¥ h√¨nh v√†o c√°c b√†i to√°n th·ª±c t·∫ø qua mini-projects v√† case studies.

---
## III. Requirements

ƒê·ªÉ c√≥ th·ªÉ ti·∫øp c·∫≠n n·ªôi dung c·ªßa to√†n b·ªô kh√≥a h·ªçc m·ªôt c√°ch d·ªÖ d√†ng, ng∆∞·ªùi ƒë·ªçc c·∫ßn c√≥ s·∫µn m·ªôt s·ªë ki·∫øn th·ª©c n·ªÅn t·∫£ng nh∆∞:

- L·∫≠p tr√¨nh Python c∆° b·∫£n: Th√†nh th·∫°o c√°c c·∫•u tr√∫c d·ªØ li·ªáu c∆° b·∫£n, v√≤ng l·∫∑p, h√†m v√† thao t√°c x·ª≠ l√Ω d·ªØ li·ªáu v·ªõi th∆∞ vi·ªán nh∆∞ numpy v√† pandas.

- To√°n h·ªçc n·ªÅn t·∫£ng: C√≥ ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ ƒë·∫°i s·ªë tuy·∫øn t√≠nh, gi·∫£i th√≠ch v√† x√°c su·∫•t th·ªëng k√™.

- K·ªπ nƒÉng t·ª± h·ªçc v√† gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ: S·∫µn s√†ng ti·∫øp c·∫≠n t√†i li·ªáu h·ªçc thu·∫≠t, nghi√™n c·ª©u thu·∫≠t to√°n, v√† th·ª≠ nghi·ªám m√¥ h√¨nh trong m√¥i tr∆∞·ªùng th·ª±c t·∫ø.

---
## IV. Syllabus

| Dealine | Module | Lecture | Description | 
| :-----: | :----- | :------ | :---------- | 
|         | ^^Module 01. Introduction to ML & Development Environment^^ | **Lecture 01. Overview to Machine Learning** <br> {++theory++} <br> [[**notes**](https://)] [[**code**](https://)] | <li>Kh√°i ni·ªám & vai tr√≤ th·ª±c ti·ªÖn <li>Ph√¢n lo·∫°i: Gi√°m s√°t; Kh√¥ng gi√°m s√°t; TƒÉng c∆∞·ªùng <li>Th√°ch th·ª©c tri·ªÉn khai m√¥ h√¨nh | {++theory++} |
| | | **Lecture 02. Roadmap for Building ML Systems** <br>{++theory++} | <li>C√°c b∆∞·ªõc x√¢y d·ª±ng h·ªá th·ªëng <li> Y·∫øu t·ªë ·∫£nh h∆∞·ªüng: d·ªØ li·ªáu, m√¥ h√¨nh, hi·ªáu nƒÉng <li> MLOps & v√≤ng ƒë·ªùi m√¥ h√¨nh |
| | | **Lecture 03. Using Python & ML Libraries** <br>==practice== | <li>Python cho khoa h·ªçc d·ªØ li·ªáu <li>Jupyter, Anaconda, Colab <li>Th∆∞ vi·ªán c∆° b·∫£n: NumPy, pandas, matplotlib, scikit-learn |
| | | | | |
|         | ^^Module 02. Data Preprocessing & Exploratory Data Analysis^^ | **Lecture 04. Essential Concepts in Data Preprocessing** | <li>Vai tr√≤ ti·ªÅn x·ª≠ l√Ω <li>K·ªπ thu·∫≠t: l√†m s·∫°ch; ch·ªçn; bi·∫øn; t·∫°o ƒë·∫∑c tr∆∞ng <li>L·ª±a ch·ªçn ph√π h·ª£p & l·ªói th∆∞·ªùng g·∫∑p |
| | | **Lecture 05. Data Cleaning & Missing Value Handling** | <li>Vai tr√≤ l√†m s·∫°ch d·ªØ li·ªáu <li>X·ª≠ l√Ω d∆∞ th·ª´a, tr√πng l·∫∑p, ngo·∫°i l·ªá <li>Chi·∫øn l∆∞·ª£c x·ª≠ l√Ω gi√° tr·ªã thi·∫øu |
| | | **Lecture 06. Feature Selection Techniques** | <li>L·ª£i √≠ch ch·ªçn ƒë·∫∑c tr∆∞ng <li> So s√°nh c√°c ph∆∞∆°ng ph√°p <li>Th·ª±c h√†nh RFE & tinh ch·ªânh <li> ƒê√°nh gi√° hi·ªáu qu·∫£ m√¥ h√¨nh |
| | | **Lecture 07. Data Transformation Techniques** | <li>K·ªπ thu·∫≠t bi·∫øn ƒë·ªïi d·ªØ li·ªáu <li>Chu·∫©n h√≥a, r·ªùi r·∫°c h√≥a, x·ª≠ l√Ω ngo·∫°i l·ªá <li>M√£ h√≥a & bi·∫øn ƒë·ªïi ph√¢n ph·ªëi <li>T·∫°o ƒë·∫∑c tr∆∞ng m·ªõi |
| | | | | |
|         | ^^Module 03. Supervised Learning Algorithms^^ | **Lecture 08. Linear Models in Machine Learning** | <li>Linear & Logistic Regression <li>Gradient Descent, Normal Equation <li>Regularization: Ridge, Lasso, Elastic Net <li>Softmax Regression cho ph√¢n lo·∫°i ƒëa l·ªõp |
| | | **Lecture 09. Support Vector Machines** | <li>Linear SVM: Hard vs. Soft Margin <li>Kernel Trick cho b√†i to√°n phi tuy·∫øn <li>Polynomial & Gaussian RBF Kernel <li>B√†i to√°n ƒë·ªëi ng·∫´u (Dual Problem) |
| | | **Lecture 10. Decision Trees** | <li>C·∫•u tr√∫c v√† tr·ª±c quan h√≥a c√¢y <li>CART & ƒë·ªô ph·ª©c t·∫°p hu·∫•n luy·ªán <li>Gini vs. Entropy <li>Si√™u tham s·ªë & ki·ªÉm so√°t overfitting <li>H·ªìi quy & ph√¢n lo·∫°i v·ªõi c√¢y quy·∫øt ƒë·ªãnh| 
| | | **Lecture 11. Ensemble Learning & Random Forests** | <li>Nguy√™n l√Ω h·ªçc t·ªï h·ª£p (Voting, Averaging) <li>Bagging, Pasting & OOB Evaluation <li>Random Forest & Extra-Trees <li>Feature Importance t·ª´ c√¢y ng·∫´u nhi√™n <li>Boosting: AdaBoost, Gradient Boosting <li>Stacking & k·ªπ thu·∫≠t k·∫øt h·ª£p m√¥ h√¨nh | 
| | | **Lecture 12. Dimensionality Reduction & Latent Variables** | <li>Hi·ªÉu l·ªùi nguy·ªÅn chi·ªÅu (Curse of Dimensionality) <li>PCA: t·ªëi ƒëa ph∆∞∆°ng sai, t·ªëi thi·ªÉu l·ªói <li>Probabilistic PCA & Factor Analysis <li>Kernel PCA & nonlinear manifold learning <li>ICA, Autoencoder & latent representations | 
| | | | | |
|         | ^^Module 04. Unsupervised Learning Algorithms^^ | **Lecture 13. Clustering & Unsupervised Pattern Discovery** | <li>K-means v√† gi·ªõi h·∫°n c·ªßa n√≥ <li>DBSCAN v√† c√°c thu·∫≠t to√°n kh√°c <li>Ch·ªçn s·ªë c·ª•m t·ªëi ∆∞u, Semi-supervised clustering |
| | | **Lecture 14. Mixture Models & Expectation Maximization** | <li>Gaussian Mixture Models (GMM) <li>Maximum Likelihood cho mixture models <li>Thu·∫≠t to√°n EM: nguy√™n l√Ω v√† tr·ª±c quan <li>Mixtures of Bernoulli, Bayesian linear regression |
| | | | | |
|         | ^^Module 5: ML Pipelines & Deployment^^ | **Lecture 15: Pipelines & Hyperparameter Tuning** | <li>scikit-learn pipelines <li>Cross-validation <li>GridSearchCV; RandomizedSearchCV <li>Feature selection t√≠ch h·ª£p <li>ƒê√°nh gi√° & t·ªëi ∆∞u m√¥ h√¨nh |
| | | **Lecture 16. Model Deployment & Inference Optimization** | <li>Hi·ªÉu l·∫ßm ph·ªï bi·∫øn khi tri·ªÉn khai <li>Batch vs Online prediction <li>K·∫øt h·ª£p batch v√† streaming pipeline <li>T·ªëi ∆∞u m√¥ h√¨nh: pruning, quantization, distillation | 

---
## V. Materials
ph·∫ßn n√†y t·ªïng
Ph·∫ßn n√†y t·ªïng h·ª£p to√†n b·ªô t√†i li·ªáu ƒë∆∞·ª£c s·ª≠ d·ª•ng trong su·ªët kh√≥a h·ªçc, t√†i li·ªáu ƒë·ªçc th√™m v√† li√™n k·∫øt tham kh·∫£o. Ngo√†i ra, b·∫°n c≈©ng s·∫Ω t√¨m th·∫•y c√°c ngu·ªìn h·ªçc t·∫≠p m·ªü r·ªông gi√∫p c·ªßng c·ªë v√† ƒë√†o s√¢u ki·∫øn th·ª©c.

**Lecture 01. Overview to Machine Learning:**

- Book: [**Machine Learning c∆° b·∫£n - V≈© H·ªØu Ti·ªáp**](https://) - Ch∆∞∆°ng 1
- Book: [**Design Machine Learning Systems - Chip Huyen**](https://)